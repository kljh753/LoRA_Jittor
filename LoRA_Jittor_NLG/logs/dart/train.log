[38;5;2m[i 0812 13:30:59.514193 20 compiler.py:956] Jittor(1.3.10.0) src: /home/dmw/anaconda3/envs/lora/lib/python3.9/site-packages/jittor[m
[38;5;2m[i 0812 13:30:59.522535 20 compiler.py:957] g++ at /usr/bin/g++(11.4.0)[m
[38;5;2m[i 0812 13:30:59.522678 20 compiler.py:958] cache_path: /home/dmw/.cache/jittor/jt1.3.10/g++11.4.0/py3.9.23/Linux-6.6.87.2x0f/12thGenIntelRCx5f/1556/default[m
[38;5;2m[i 0812 13:30:59.547756 20 __init__.py:412] Found nvcc(12.1.66) at /usr/local/cuda-12.1/bin/nvcc.[m
[38;5;2m[i 0812 13:30:59.740473 20 __init__.py:412] Found addr2line(2.38) at /usr/bin/addr2line.[m
[38;5;2m[i 0812 13:30:59.989576 20 compiler.py:1013] cuda key:cu12.1.66[m
[38;5;2m[i 0812 13:31:01.410327 20 __init__.py:227] Total mem: 7.61GB, using 2 procs for compiling.[m
[38;5;2m[i 0812 13:31:01.887199 20 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0812 13:31:02.491553 20 init.cc:63] Found cuda archs: [86,][m
[38;5;3m[w 0812 13:31:05.233218 20 compile_extern.py:203] CUDA related path found in LD_LIBRARY_PATH or PATH, This path may cause jittor found the wrong libs, please unset LD_LIBRARY_PATH and remove cuda lib path in Path. 
Or you can let jittor install cuda for you: `python3.x -m jittor_utils.install_cuda`[m
[38;5;2m[i 0812 13:31:06.809488 20 cuda_flags.cc:55] CUDA enabled.[m
[38;5;3m[w 0812 13:31:24.129308 20 grad.cc:81] grads[20] 'transformer.h.10.attn.c_attn.lora_A' doesn't have gradient. It will be set to zero: Var(238:12:24:12:i0:o12:s1:n0:g1,float32,transformer.h.10.attn.c_attn.lora_A,5068f1000)[8,768,][m
[38;5;3m[w 0812 13:31:24.129490 20 grad.cc:81] grads[21] 'transformer.h.10.attn.c_attn.lora_B' doesn't have gradient. It will be set to zero: Var(244:12:24:12:i0:o12:s1:n1:g1,float32,transformer.h.10.attn.c_attn.lora_B,5068f7000)[1536,4,][m
[38;5;2m[i 0812 13:31:46.385834 20 cuda_device_allocator.cc:30] 
=== display_memory_info ===
 total_cpu_ram: 7.611GB total_device_ram:     6GB
 hold_vars: 1013 lived_vars: 20010 lived_ops: 16316
 name: sfrl is_device: 1 used: 12.08GB(96.8%) unused: 406.3MB(3.18%) ULB:   4.5MB ULBO:     6MB total: 12.48GB
 name: sfrl is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used: 1.586MB(52.9%) unused: 1.414MB(47.1%) total:     3MB
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 1 used:     0 B(0%) unused:   512 B(100%) total:   512 B
 cpu&gpu: 12.48GB gpu: 12.48GB cpu:     3MB
 free: cpu( 6.15GB) gpu(    0 B)
 swap: total(    0 B) last(    0 B)
===========================
[m
[38;5;2m[i 0812 13:31:47.004016 20 cuda_device_allocator.cc:30] 
=== display_memory_info ===
 total_cpu_ram: 7.611GB total_device_ram:     6GB
 hold_vars: 1013 lived_vars: 20010 lived_ops: 16316
 name: sfrl is_device: 1 used: 12.08GB(96.8%) unused: 406.3MB(3.18%) ULB:   4.5MB ULBO:     6MB total: 12.48GB
 name: sfrl is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used: 1.586MB(52.9%) unused: 1.414MB(47.1%) total:     3MB
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 cpu&gpu: 12.48GB gpu: 12.48GB cpu:     3MB
 free: cpu( 6.15GB) gpu(    0 B)
 swap: total(    0 B) last(    0 B)
===========================
[m
[38;5;1m[e 0812 13:31:47.005339 20 executor.cc:682] 
=== display_memory_info ===
 total_cpu_ram: 7.611GB total_device_ram:     6GB
 hold_vars: 1013 lived_vars: 20010 lived_ops: 16316
 name: sfrl is_device: 1 used: 12.08GB(96.8%) unused: 406.3MB(3.18%) ULB:   4.5MB ULBO:     6MB total: 12.48GB
 name: sfrl is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: sfrl is_device: 0 used: 1.586MB(52.9%) unused: 1.414MB(47.1%) total:     3MB
 name: sfrl is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 0 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 name: temp is_device: 1 used:     0 B(-nan%) unused:     0 B(-nan%) total:     0 B
 cpu&gpu: 12.48GB gpu: 12.48GB cpu:     3MB
 free: cpu( 6.15GB) gpu(    0 B)
 swap: total(    0 B) last(    0 B)
===========================
[m
Using CUDA device
myrank: 0 local_rank: 0 device_count: 1 world_size: 1
====================================================================================================
        - platform : single
        - local_rank : 0
        - rank : 0
        - device : 0
        - world_size : 1
        - random_seed : 100
        - lr : 0.0002
        - weight_decay : 0.01
        - correct_bias : True
        - adam_epislon : 1e-06
        - no_decay_bias : False
        - adam_beta1 : 0.9
        - adam_beta2 : 0.999
        - scheduler : linear
        - max_step : None
        - max_epoch : 3
        - warmup_step : 500
        - i_steps : 0
        - i_lrs : 0.00025
        - train_data : ./data/dart/train.jsonl
        - valid_data : ./data/dart/valid.jsonl
        - train_batch_size : 4
        - valid_batch_size : 1
        - grad_acc : 1
        - clip : 0.0
        - seq_len : 128
        - model_card : gpt2.sm
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin
        - fp16 : False
        - log_interval : 20
        - eval_interval : 100
        - save_interval : 1000
        - work_dir : ./trained_models/GPT2_M/dart
        - lora_dim : 4
        - lora_alpha : 32
        - obj : clm
        - lora_dropout : 0.1
        - label_smooth : 0.1
        - roll_interval : -1
        - roll_lr : 1e-05
        - roll_step : 100
        - eval_epoch : 1
====================================================================================================
Experiment dir : ./trained_models/GPT2_M/dart
batch_size: 4
batch_size: 1
set max_step: 375
loading model pretrained weight.
start to train the model................ 1
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/dmw/anaconda3/envs/lora/lib/python3.9/linecache.py", line 46, in getlines
    return updatecache(filename, module_globals)
  File "/home/dmw/anaconda3/envs/lora/lib/python3.9/linecache.py", line 137, in updatecache
    lines = fp.readlines()
  File "/home/dmw/anaconda3/envs/lora/lib/python3.9/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 3723: invalid continuation byte

Original exception was:
Traceback (most recent call last):
  File "/mnt/d/é›·ç”µå¤§é­”çŽ‹/LoRA_Jittor/LoRA_Jittor_NLG/src/gpt2_ft.py", line 386, in <module>
  File "/mnt/d/é›·ç”µå¤§é­”çŽ‹/LoRA_Jittor/LoRA_Jittor_NLG/src/gpt2_ft.py", line 368, in main
  File "/mnt/d/é›·ç”µå¤§é­”çŽ‹/LoRA_Jittor/LoRA_Jittor_NLG/src/gpt2_ft.py", line 209, in train_validate
  File "/mnt/d/é›·ç”µå¤§é­”çŽ‹/LoRA_Jittor/LoRA_Jittor_NLG/src/gpt2_ft.py", line 201, in forward_step
RuntimeError: Wrong inputs arguments, Please refer to examples(help(jt.item)).

Types of your inputs are:
 self	= Var,
 args	= (),

The function declarations are:
 ItemData item()

Failed reason:[38;5;1m[f 0812 13:31:47.006169 20 mem_info.cc:272] 
*******************
GPU memory is overflow, please reduce your batch_size or data size!
Total:     6GB Used: 12.48GB[m
